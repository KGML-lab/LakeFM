_target_: lakefm.model.LakeFMModule

model_name: lakefm

# Encoder
d_model: 128
num_layers: 6
n_heads: 4

# Variate, depth and input embedding dimension
# d_model = var_embed_dim + depth_embed_dim + inp_embed_dim if using add_or_concat = "concat"
# if using add_or_concat = "add", update only d_model, remaining three will be ignored
var_embed_dim: 64
depth_embed_dim: 32
inp_embed_dim: 64
time_embed_dim: 16 # sin-cos

# static and temporal dimension
static_dim: 32
temporal_dim: 96

# Attention & Feedforward
output_attention: false
factor: 1
dropout_p: 0.0
head_dropout: 0.02
d_ff: 2048
attn_dropout_p: 0.0
activation: swiglu

revin: true

tokenization: scalar # options: scalar, patch, temporal
patch_size: 3

# vocabulary
max_vars: 24
pad_token: 0
max_seq_len: 3076
max_pred_len: 588
seq_len: ${seq_len}   
pred_len: ${pred_len}   

# add contextual information
add_or_concat: "concat"

# additional linear layer in the forecast head
additional_forecast_layer: true

# fourier feature encoding
num_bands: 6
max_resolution: 1.0
include_input: True

# positional encoding
use_rope: True # if false, model, by default, switches to SPE

# contrastive learning
cl_proj_type: attention_pooling # mean_pooling
cl_proj_dim: 64

# use time embed
use_time_embed: True

# use pre-norm
use_pre_norm: true

# Probabilistic forecasting: variate-wise degrees of freedom
# If true: each variable gets a learnable base df + context-aware adjustment (hybrid approach)
# If false: single learnable df per token (current default)
variate_wise_df: false

# Degrees of freedom base source
# If true: reuse var_id_embed with a linear projector (no gradient back to var_id_embed when stop_grad is true)
# If false: use a separate variate_df_embedding
shared_variate_embedding_for_df: false
# When using shared embedding, stop gradients into var_id_embed for df path
stop_grad_on_shared_var_embed_for_df: true

# Optional scheduling logic
num_warmup_steps: 10000